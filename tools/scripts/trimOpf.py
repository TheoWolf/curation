# Python 2 is the only version installed on Databrary server
import sys
import os
import json
import logging
import argparse
import math
from similarity.jarowinkler import JaroWinkler
from utils import dbapi
from shutil import copyfile

try:
    import pyvyu as pv  # Python 3 package
except ImportError:
    import py2vyu as pv  # Python 2 package
from utils import csv_helpers as file_utils

logger = logging.getLogger('logs')
logger.setLevel(logging.DEBUG)

fh = logging.FileHandler('../../logs/all.log')
ch = logging.StreamHandler()

formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(filename)s.%(funcName)s - %(message)s')

fh.setFormatter(formatter)
ch.setFormatter(formatter)

logger.addHandler(ch)
logger.addHandler(fh)

parser = argparse.ArgumentParser(
    description='Command line tool used internally to find OPF files in a directory and trim the latter according to '
                'the video clips found in the JSON ingest file, if volume, username and password the script will attempt '
                'an upload to Databrary')
parser.add_argument('input', help='Path to the Ingest JSON file. Is required', required=True)
parser.add_argument(
    '-u', '--username', help='Databrary username', type=str, dest='__username', required=False)
parser.add_argument('-p', '--password',
                    help='Databrary password', type=str, dest='__password', required=False)
parser.add_argument('-v', '--volume', help='Volume ID', type=int, dest='__volume',required=False)
parser.add_argument('-f', '--format', help='File Format', type=str,
                    default='json', choices=['json', 'opf'], required=False)
parser.add_argument(
    '-on', '--onset', help='ONSET in ms. Is required', type=int, required=False)
parser.add_argument('-off', '--offset',
                    help='OFFSET in ms. Is required', type=int, required=False)
parser.add_argument('-c', '--columns', nargs='+', type=str,
                    help='Columns to be edited. Not required', required=False)

args = parser.parse_args()

if args.format == 'opf' and (args.onset is None or args.offset is None):
    parser.error('OPF input require an onset --onset and offset --offset')

_input_file = args.input  # Input File
_is_json = args.format == 'json'

if args.__volume is not None and (args.__username is not None or args.__password is not None) :
    logger.error('Username and password are required with Volume argument')
    sys.exit()


if args.onset is not None and args.offset is not None:
    _onset_input = args.onset  # onset in ms
    _offset_input = args.offset  # offset in ms

_video_extensions = [".webm", ".mpg", ".mp4",
                     ".mov", ".mts", ".avi", ".wmv", ".dv"]
_audio_extensions = [".wav", ".aac", ".wma", ".mp3"]
_edit_columns = args.columns
_exception = ["id"]
_columns_to_trim = [col for col in _edit_columns if col not in _exception]
_opf_extensions = [".opf"]


def parseInputFile(input_file, is_json):
    if is_json:
        parseIngestFile(input_file)
    elif not is_json and _onset_input and _offset_input:
        parseAndTrimOpf(_input_file, _edit_columns, _onset_input, _offset_input)


def parseIngestFile(ingest_json_path):
    """
    Parse a JSON ingest file generated by csv2json.py script. The function will detect
    assets in each container and cut OPF files according to Media clips if there is any.
    Error will be returned if the OPF file path is invalid.
    """
    with open(ingest_json_path) as json_file:
        ingest_data = json.load(json_file)

    if ingest_data:
        logger.info('Ingest file %s loaded', ingest_json_path)
    else:
        logger.error('Ingest file %s not loaded', ingest_json_path)

    # container is a session in Databrary
    containers_list = ingest_data['containers']

    if containers_list:
        logger.debug('Found containers in ingest file')
    else:
        logger.warning(
            'Cannot find containers in %s, the program will exit', ingest_json_path)
        sys.exit()

    for i, container in enumerate(containers_list):
        assets = container['assets']
        if len(assets) > 1:
            logger.info('Found %d assets in container #%d', len(assets), i)
        elif len(assets) <= 1:
            logger.warning(
                'Not enough asset %d in container %d passing to the next one', len(assets), i)
            continue

        onset = float('nan')
        offset = float('nan')
        has_media = False
        valid_clips = False
        opf_files_list = []
        for j, asset in enumerate(assets):
            # Find if the asset is a video file or opf
            asset_path = asset['file']
            if isMedia(asset_path):
                logger.info("Asset %s is a media", asset_path)
                has_media = True
                if len(asset['clip']) > 0:
                    clip = asset['clip']
                    if len(clip) != 2:
                        logger.error(
                            'error in container #%d asset #%d; clips require two values [onset, offset]', i, j)
                        valid_clips = False
                        break
                    if math.isnan(onset) and math.isnan(offset):
                        valid_clips = True
                        logger.info('container #%d asset #%d clip found %d-%d', i, i, int(clip[0]), int(clip[1]))
                        onset = int(clip[0])
                        offset = int(clip[1])
                    elif clip[0] != onset or clip[1] != offset:
                        logger.error(
                            'error in container #%d asset #%d; All Media clips must have the same onset and offset', i,
                            j)
                        valid_clips = False
                        break
            elif isOpf(asset_path):
                logger.info("Asset %s is an opf file", asset_path)
                opf_files_list.append(asset_path)
            else:
                logger.warning("Cannot find any Media or OPF file")

        if opf_files_list is not None and has_media and valid_clips:
            for opf_file in opf_files_list:
                opf_cut = parseAndTrimOpf(opf_file, _edit_columns, onset, offset)
                if args.__username is not None & args.__password is not None & args.__volume is not None:
                    uploadOpf(opf_cut, args.__volume)


def parseAndTrimOpf(opf_path, columns_list, onset, offset):
    """
    Parse and cut OPF file according to an onset and offset passed via arguments or
    found in the ingest JSON file.
    """
    opf_file_orig = os.path.realpath(opf_path)
    opf_path_cut = os.path.splitext(opf_file_orig)[0] + '_cut.opf'
    opf_file = copyfile(opf_file_orig, opf_path_cut)
    sheet = pv.load_opf(opf_file)
    if sheet.get_column_list() < 1:
        logger.error('OPF file is empty')
        return None
    if columns_list is not None:
        sheet.columns = {colname: col for (colname, col) in sheet.columns.items() if colname in columns_list}

    for colname, col in sheet.columns.items():
        for col in [sheet.columns[c] for c in _columns_to_trim]:
            col.cells = [cell for cell in col.cells if cell.onset >= onset and cell.offset <= offset]

    for colname, col in sheet.columns.items():
        for cell in col.cells:
            cell.onset = max(cell.onset - onset, 0)
            cell.offset = cell.offset - onset
    pv.save_opf(sheet, opf_path_cut, *sheet.columns.keys())
    return opf_path_cut


def uploadOpf(file_path, volume):
    jaroWinkler = JaroWinkler()
    api = dbapi.DatabraryApi(args.__username, args.__password, False)
    volume_assets = api.get_volume_assets(volume)
    for session in volume_assets:
        session_assets = session["assets"]
        for asset in session_assets:
            asset_name = asset['name'] if "." not in asset['name'] else asset['name'][0:-4]
            opf_name = dbapi.DatabraryApi.getFileName(file_path) if "." not in dbapi.DatabraryApi.getFileName(
                file_path) else dbapi.DatabraryApi.getFileName(file_path)[0:-4]
            if jaroWinkler.similarity(asset_name.lower(), opf_name.lower()) >= 0.8:
                api.upload_asset(volume, session_assets['id'], file_path)


def isOpf(asset_file_path):
    file_extension = file_utils.getFileExtension(asset_file_path)
    logger.debug('File Path: %s - File Ext: %s',
                 asset_file_path, file_extension)
    if file_extension is not None and file_extension in _opf_extensions:
        return True
    else:
        return False


def isMedia(asset_file_path):
    file_extension = file_utils.getFileExtension(asset_file_path)
    logger.debug('File Path: %s - File Ext: %s',
                 asset_file_path, file_extension)
    if file_extension in _video_extensions or file_extension in _audio_extensions:
        return True
    else:
        return False


if __name__ == '__main__':
    parseInputFile(_input_file, _is_json)
