# Python 2 is the only version installed on Databrary server
import sys
import os
import json
import logging
import argparse
import textdistance
from utils import dbapi
from shutil import copyfile
from utils import csv_helpers as utils

try:
    import pyvyu as pv  # Python 3 package
except ImportError:
    import py2vyu as pv  # Python 2 package
from utils import csv_helpers as file_utils

BASE_DIR = os.path.dirname(os.path.abspath('../'))
CONFIG_DIR = os.path.join(BASE_DIR, 'config', )
OPF_DIR = os.path.join(BASE_DIR, 'tools', 'opfs')
INPUT_DIR = os.path.join(BASE_DIR, 'tools', 'input')
LOGS_DIR = os.path.join(BASE_DIR, 'logs')

logger = logging.getLogger('logs')
logger.setLevel(logging.DEBUG)

fh = logging.FileHandler(os.path.join(LOGS_DIR, 'all.log'))
ch = logging.StreamHandler()

formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(filename)s.%(funcName)s - %(message)s')

fh.setFormatter(formatter)
ch.setFormatter(formatter)

logger.addHandler(ch)
logger.addHandler(fh)

parser = argparse.ArgumentParser(
    description='Command line tool used internally to find OPF files in a directory and trim the latter according to '
                'the video clips found in the JSON ingest file, if volume, username and password the script will attempt '
                'an upload to Databrary')
parser.add_argument('input', help='Path to the Ingest JSON file. Is required')
parser.add_argument('-v', '--volume', help='Volume ID', type=int, dest='__volume', required=False)
parser.add_argument('-f', '--format', help='File Format', type=str,
                    default='json', choices=['json', 'opf'], required=False)
parser.add_argument(
    '-on', '--onset', help='ONSET in ms. Is required', type=int, required=False)
parser.add_argument('-off', '--offset',
                    help='OFFSET in ms. Is required', type=int, required=False)
parser.add_argument('-c', '--columns', nargs='+', type=str,
                    help='Columns to be edited. Not required', required=False)

args = parser.parse_args()

if args.format == 'opf' and (args.onset is None or args.offset is None):
    parser.error('OPF input require an onset --onset and offset --offset')

_input_file = args.input  # Input File
_is_json = args.format == 'json'

if args.__volume is not None:
    with open(os.path.join(CONFIG_DIR, 'credentials.json')) as creds:
        __credentials = json.load(creds)
        __username = __credentials['username']
        __password = __credentials['password']
        if __credentials is None:
            logger.error('Cannot find Databrary credentials')
            sys.exit()
        try:
            api = dbapi.DatabraryApi(__username, __password)
            volume_assets = api.get_volume_assets(args.__volume)
        except AttributeError as e:
            logger.error(e)
            sys.exit()

if args.onset is not None and args.offset is not None:
    _onset_input = args.onset  # onset in ms
    _offset_input = args.offset  # offset in ms

_video_extensions = [".webm", ".mpg", ".mp4",
                     ".mov", ".mts", ".avi", ".wmv", ".dv"]
_audio_extensions = [".wav", ".aac", ".wma", ".mp3"]
_opf_extensions = [".opf"]
_edit_columns = args.columns
_exception = ["id"]

def parseInputFile(input_file, is_json):
    if is_json:
        parseIngestFile(input_file)
    elif not is_json and _onset_input and _offset_input:
        parseAndTrimOpf(_input_file, _edit_columns, _onset_input, _offset_input)


def findOpf(asset_name):
    max_similarity = 0
    opf_similar = None
    for root, dirs, files in os.walk(OPF_DIR):
        for file in files:
            if file.endswith(".opf"):
                opf_name = dbapi.DatabraryApi.getFileName(file)[0:-4]
                text_similarity = textdistance.jaro_winkler.similarity(asset_name.lower(), opf_name.lower())
                if text_similarity > max_similarity:
                    max_similarity = text_similarity
                    opf_similar = file

    logger.info('file %s and asset %s similarity is text_similarity %s', opf_similar, asset_name, str(max_similarity))
    return os.path.join(root, opf_similar)


def parseIngestFile(ingest_json_path):
    """
    Parse a JSON ingest file generated by csv2json.py script. The function will detect
    assets in each container and cut OPF files according to Media clips if there is any.
    Error will be returned if the OPF file path is invalid.
    """
    with open(ingest_json_path) as json_file:
        ingest_data = json.load(json_file)

    if ingest_data:
        logger.info('Ingest file %s loaded', ingest_json_path)
    else:
        logger.error('Ingest file %s not loaded', ingest_json_path)

    # container is a session in Databrary
    containers_list = ingest_data['containers']

    if containers_list:
        logger.debug('Found containers in ingest file')
    else:
        logger.warning(
            'Cannot find containers in %s, the program will exit', ingest_json_path)
        sys.exit()

    for i, container in enumerate(containers_list):
        assets = container['assets']
        if len(assets) >= 1:
            logger.info('Found %d assets in container #%d', len(assets), i)
        else:
            logger.warning(
                'Not enough asset %d in container %d passing to the next one', len(assets), i)
            continue

        for j, asset in enumerate(assets):
            # Find if the asset is a video file or opf
            asset_path = asset['file']
            if isMedia(asset_path):
                logger.info("Asset %s is a media", asset_path)
                if len(asset['clip']) > 0:
                    clip = asset['clip']
                    if len(clip) != 2:
                        logger.error(
                            'error in container #%d asset #%d; clips require two values [onset, offset]', i, j)
                        break
                    else:
                        logger.info('container #%d asset #%d clip found %d-%d', i, i, int(clip[0]), int(clip[1]))
                        onset = int(clip[0])
                        offset = int(clip[1])
                        asset_name = dbapi.DatabraryApi.getFileName(
                            asset_path) if "." not in dbapi.DatabraryApi.getFileName(
                            asset_path) else dbapi.DatabraryApi.getFileName(asset_path)[0:-4]
                        opf_path = findOpf(asset_name)
                        opf_cut = parseAndTrimOpf(opf_path, _edit_columns, onset, offset)
                        if args.__volume is not None and opf_path is not None:
                            uploadOpf(opf_cut, asset_name, args.__volume)
            else:
                logger.warning("Cannot find any Media or OPF file")


def parseAndTrimOpf(opf_path, columns_list, onset, offset):
    """
    Parse and cut OPF file according to an onset and offset passed via arguments or
    found in the ingest JSON file.
    """
    logger.info("Trim opf %s from %d to %d", opf_path, onset, offset)
    opf_file_orig = os.path.realpath(opf_path)
    opf_path_cut = os.path.splitext(opf_file_orig)[0] + '_cut.opf'
    copyfile(opf_file_orig, opf_path_cut)
    sheet = pv.load_opf(opf_path_cut)
    if sheet.get_column_list() < 1:
        logger.error('OPF file is empty')
        return None
    if columns_list is not None:
        sheet.columns = {colname: col for (colname, col) in sheet.columns.items() if colname in columns_list}

    _columns_to_trim = [col for col in sheet.columns if col not in _exception]

    for colname, col in sheet.columns.items():
        for col in [sheet.columns[c] for c in _columns_to_trim]:
            col.cells = [cell for cell in col.cells if cell.onset >= onset and cell.offset <= offset]

    for colname, col in sheet.columns.items():
        for cell in col.cells:
            cell.onset = max(cell.onset - onset, 0)
            cell.offset = cell.offset - onset
    pv.save_opf(sheet, opf_path_cut, *sheet.columns.keys())
    return opf_path_cut


def uploadOpf(file_path, asset_name, volume):
    if volume_assets:
        for session in volume_assets:
            session_assets = session["assets"]
            for asset in session_assets:
                asset_name_volume = asset['name'] if "." not in asset['name'] else asset['name'][0:-4]
                if asset_name == asset_name_volume:
                    logger.debug("Uploading file %s to session %s", file_path, str(session['id']))
                    api.upload_asset(volume, session['id'], file_path)


def isOpf(asset_file_path):
    file_extension = file_utils.getFileExtension(asset_file_path)
    logger.debug('File Path: %s - File Ext: %s',
                 asset_file_path, file_extension)
    if file_extension is not None and file_extension in _opf_extensions:
        return True
    else:
        return False


def isMedia(asset_file_path):
    file_extension = file_utils.getFileExtension(asset_file_path)
    logger.debug('File Path: %s - File Ext: %s',
                 asset_file_path, file_extension)
    if file_extension in _video_extensions or file_extension in _audio_extensions:
        return True
    else:
        return False


if __name__ == '__main__':
    parseInputFile(_input_file, _is_json)
